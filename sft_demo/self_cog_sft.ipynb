{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c202aad6-e74a-4dd0-8c7b-d706af1cefc6",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T07:39:22.027662Z",
     "iopub.status.busy": "2024-07-23T07:39:22.027349Z",
     "iopub.status.idle": "2024-07-23T07:40:31.393764Z",
     "shell.execute_reply": "2024-07-23T07:40:31.393143Z",
     "shell.execute_reply.started": "2024-07-23T07:39:22.027643Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 15:39:26.377384: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-23 15:39:26.389224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-23 15:39:26.403322: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-23 15:39:26.407565: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-23 15:39:26.418226: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 15:39:27.235483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/local/lib/python3.10/site-packages/_distutils_hack/__init__.py:55: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "[INFO:swift] Successfully registered `/mnt/workspace/swift/swift/llm/data/dataset_info.json`\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: qwen/Qwen-1_8B-Chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template_type: qwen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING:modelscope] Using the master branch is fragile, please use it with caution!\n",
      "[INFO:modelscope] Use user-specified model revision: master\n",
      "Downloading: 100%|██████████| 8.21k/8.21k [00:00<00:00, 15.3kB/s]\n",
      "Downloading: 100%|██████████| 50.8k/50.8k [00:00<00:00, 99.7kB/s]\n",
      "Downloading: 100%|██████████| 910/910 [00:00<00:00, 1.77kB/s]\n",
      "Downloading: 100%|██████████| 77.0/77.0 [00:00<00:00, 130B/s]\n",
      "Downloading: 100%|██████████| 2.29k/2.29k [00:00<00:00, 3.78kB/s]\n",
      "Downloading: 100%|██████████| 1.88k/1.88k [00:00<00:00, 2.65kB/s]\n",
      "Downloading: 100%|██████████| 249/249 [00:00<00:00, 403B/s]\n",
      "Downloading: 100%|██████████| 7.11k/7.11k [00:00<00:00, 14.6kB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 126kB/s]\n",
      "Downloading: 100%|██████████| 1.90G/1.90G [00:05<00:00, 356MB/s] \n",
      "Downloading: 100%|██████████| 1.52G/1.52G [00:06<00:00, 270MB/s] \n",
      "Downloading: 100%|██████████| 14.4k/14.4k [00:00<00:00, 23.9kB/s]\n",
      "Downloading: 100%|██████████| 54.3k/54.3k [00:00<00:00, 106kB/s]\n",
      "Downloading: 100%|██████████| 15.0k/15.0k [00:00<00:00, 26.8kB/s]\n",
      "Downloading: 100%|██████████| 2.44M/2.44M [00:00<00:00, 3.79MB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 27.6kB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 140kB/s]\n",
      "Downloading: 100%|██████████| 11.6k/11.6k [00:00<00:00, 23.8kB/s]\n",
      "Downloading: 100%|██████████| 302k/302k [00:00<00:00, 540kB/s]\n",
      "Downloading: 100%|██████████| 615k/615k [00:00<00:00, 1.10MB/s]\n",
      "Downloading: 100%|██████████| 25.9k/25.9k [00:00<00:00, 51.1kB/s]\n",
      "Downloading: 100%|██████████| 9.39k/9.39k [00:00<00:00, 15.8kB/s]\n",
      "Downloading: 100%|██████████| 173/173 [00:00<00:00, 271B/s]\n",
      "Downloading: 100%|██████████| 66.8k/66.8k [00:00<00:00, 120kB/s]\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat\n",
      "[INFO:swift] model_kwargs: {'device_map': 'auto'}\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0f97109a4f43c3aadcce1239b6ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] model.max_model_len: 8192\n",
      "[INFO:swift] Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query: 介绍你自己\n",
      "response: 我是来自阿里云的大规模语言模型，我叫通义千问。我的目标是为用户提供最好的服务体验。我可以回答各种问题、提供定义、解释和建议，还能表达观点、撰写代码以及开发算法。如果你有任何问题或需要帮助，请随时告诉我，我会尽力为你提供支持。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from swift.llm import (\n",
    "    get_model_tokenizer, get_template, inference, ModelType,\n",
    "    get_default_template_type, inference_stream\n",
    ")\n",
    "from swift.utils import seed_everything\n",
    "import torch\n",
    "\n",
    "model_type = ModelType.qwen_1_8b_chat\n",
    "template_type = get_default_template_type(model_type)\n",
    "print(f'template_type: {template_type}')  # template_type: qwen\n",
    "\n",
    "\n",
    "kwargs = {}\n",
    "# kwargs['use_flash_attn'] = True  # 使用flash_attn\n",
    "\n",
    "model, tokenizer = get_model_tokenizer(model_type, torch.float16,\n",
    "                                       model_kwargs={'device_map': 'auto'}, **kwargs)\n",
    "# 修改max_new_tokens\n",
    "model.generation_config.max_new_tokens = 128\n",
    "\n",
    "template = get_template(template_type, tokenizer)\n",
    "seed_everything(42)\n",
    "query = '介绍你自己'\n",
    "response, history = inference(model, template, query)\n",
    "print(f'query: {query}')\n",
    "print(f'response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "141c6415-76a6-43f6-b3a1-ec666f670418",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T07:52:59.220527Z",
     "iopub.status.busy": "2024-07-23T07:52:59.220188Z",
     "iopub.status.idle": "2024-07-23T07:52:59.258131Z",
     "shell.execute_reply": "2024-07-23T07:52:59.257530Z",
     "shell.execute_reply.started": "2024-07-23T07:52:59.220493Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `kenan` has already been registered in the DATASET_MAPPING.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m         response\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HfDataset\u001b[38;5;241m.\u001b[39mfrom_dict({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m: query, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m: response})\n\u001b[0;32m---> 28\u001b[0m \u001b[43mregister_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCustomDatasetName\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkenan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mswift/kenan.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_preprocess_stsb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_dataset_from_repo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# test dataset\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m get_dataset([CustomDatasetName\u001b[38;5;241m.\u001b[39mkenan],\n\u001b[1;32m     34\u001b[0m                                              check_dataset_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarning\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/workspace/swift/swift/llm/utils/dataset.py:220\u001b[0m, in \u001b[0;36mregister_dataset\u001b[0;34m(dataset_name, dataset_id_or_path, subsets, preprocess_func, get_function, split, hf_dataset_id, function_kwargs, exist_ok, is_local, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     preprocess_func \u001b[38;5;241m=\u001b[39m SmartPreprocessor()\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m DATASET_MAPPING:\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` has already been registered in the DATASET_MAPPING.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subsets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     subsets \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: The `kenan` has already been registered in the DATASET_MAPPING."
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "from datasets import Dataset as HfDataset\n",
    "from modelscope import MsDataset\n",
    "\n",
    "from swift.llm import get_dataset, register_dataset, get_dataset_from_repo\n",
    "from swift.utils import get_logger\n",
    "\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "class CustomDatasetName:\n",
    "    kenan = 'kenan'\n",
    "\n",
    "def _preprocess_stsb(dataset: HfDataset) -> HfDataset:\n",
    "    prompt = \"\"\"Task: Based on the given two sentences, provide a similarity score between 0.0 and 5.0.\n",
    "Sentence 1: {text1}\n",
    "Sentence 2: {text2}\n",
    "Similarity score: \"\"\"\n",
    "    query = []\n",
    "    response = []\n",
    "    for d in dataset:\n",
    "        query.append(prompt.format(text1=d['text1'], text2=d['text2']))\n",
    "        response.append(f\"{d['label']:.1f}\")\n",
    "    return HfDataset.from_dict({'query': query, 'response': response})\n",
    "\n",
    "\n",
    "register_dataset(CustomDatasetName.kenan, 'swift/kenan.json', None, _preprocess_stsb, get_dataset_from_repo)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # test dataset\n",
    "    train_dataset, val_dataset = get_dataset([CustomDatasetName.kenan],\n",
    "                                             check_dataset_strategy='warning')\n",
    "    print(f'train_dataset: {train_dataset}')\n",
    "    print(f'val_dataset: {val_dataset}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6db034-50e9-42e6-9ef5-6253bcbbf3ed",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T07:57:40.726149Z",
     "iopub.status.busy": "2024-07-23T07:57:40.725829Z",
     "iopub.status.idle": "2024-07-23T08:01:00.263776Z",
     "shell.execute_reply": "2024-07-23T08:01:00.263103Z",
     "shell.execute_reply.started": "2024-07-23T07:57:40.726128Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Setting template_type: qwen\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] Setting args.dataloader_num_workers: 1\n",
      "[INFO:swift] output_dir: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740\n",
      "[INFO:swift] Start time of running main: 2024-07-23 15:57:40.740586\n",
      "[INFO:swift] args: SftArguments(model_type='qwen-1_8b-chat', model_id_or_path='qwen/Qwen-1_8B-Chat', model_revision='master', sft_type='lora', freeze_parameters=0.0, additional_trainable_parameters=[], tuner_backend='peft', template_type='qwen', output_dir='/mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740', add_output_dir_suffix=True, ddp_backend=None, ddp_find_unused_parameters=None, ddp_broadcast_buffers=None, seed=42, resume_from_checkpoint=None, resume_only_model=False, ignore_data_skip=False, dtype='bf16', packing=False, dataset=['alpaca-zh#500', 'kenan.json#500'], val_dataset=[], dataset_seed=42, dataset_test_ratio=0.01, use_loss_scale=False, loss_scale_config_path='/mnt/workspace/swift/swift/llm/agent/default_loss_scale_config.json', system=None, tools_prompt='react_en', max_length=2048, truncation_strategy='delete', check_dataset_strategy='none', model_name=[None, None], model_author=[None, None], quant_method=None, quantization_bit=0, hqq_axis=0, hqq_dynamic_config_path=None, bnb_4bit_comp_dtype='bf16', bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_quant_storage=None, lora_target_modules=[], lora_target_regex=None, lora_rank=8, lora_alpha=32, lora_dropout_p=0.05, lora_bias_trainable='none', lora_modules_to_save=[], lora_dtype='AUTO', lora_lr_ratio=None, use_rslora=False, use_dora=False, init_lora_weights='true', rope_scaling=None, boft_block_size=4, boft_block_num=0, boft_n_butterfly_factor=1, boft_target_modules=['DEFAULT'], boft_dropout=0.0, boft_modules_to_save=[], vera_rank=256, vera_target_modules=['DEFAULT'], vera_projection_prng_key=0, vera_dropout=0.0, vera_d_initial=0.1, vera_modules_to_save=[], adapter_act='gelu', adapter_length=128, use_galore=False, galore_rank=128, galore_target_modules=None, galore_update_proj_gap=50, galore_scale=1.0, galore_proj_type='std', galore_optim_per_parameter=False, galore_with_embedding=False, galore_quantization=False, galore_proj_quant=False, galore_proj_bits=4, galore_proj_group_size=256, galore_cos_threshold=0.4, galore_gamma_proj=2, galore_queue_size=5, adalora_target_r=8, adalora_init_r=12, adalora_tinit=0, adalora_tfinal=0, adalora_deltaT=1, adalora_beta1=0.85, adalora_beta2=0.85, adalora_orth_reg_weight=0.5, ia3_target_modules=['DEFAULT'], ia3_feedforward_modules=[], ia3_modules_to_save=[], llamapro_num_new_blocks=4, llamapro_num_groups=None, neftune_noise_alpha=None, neftune_backend='transformers', lisa_activated_layers=0, lisa_step_interval=20, gradient_checkpointing=True, deepspeed=None, batch_size=1, eval_batch_size=1, num_train_epochs=1, max_steps=-1, optim='adamw_torch', adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, learning_rate=0.0001, weight_decay=0.1, gradient_accumulation_steps=16, max_grad_norm=0.5, predict_with_generate=False, lr_scheduler_type='cosine', lr_scheduler_kwargs={}, warmup_ratio=0.05, warmup_steps=0, eval_steps=50, save_steps=50, save_only_model=False, save_total_limit=2, logging_steps=5, acc_steps=1, dataloader_num_workers=1, dataloader_pin_memory=True, dataloader_drop_last=False, push_to_hub=False, hub_model_id=None, hub_token=None, hub_private_repo=False, push_hub_strategy='push_best', test_oom_error=False, disable_tqdm=False, lazy_tokenize=False, preprocess_num_proc=1, use_flash_attn=None, ignore_args_error=False, check_model_is_latest=True, logging_dir='/mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/runs', report_to=['tensorboard'], acc_strategy='token', save_on_each_node=False, evaluation_strategy='steps', save_strategy='steps', save_safetensors=True, gpu_memory_fraction=None, include_num_input_tokens_seen=False, local_repo_path=None, custom_register_path=None, custom_dataset_info=None, device_map_config_path=None, device_max_memory=[], max_new_tokens=2048, do_sample=True, temperature=0.3, top_k=20, top_p=0.7, repetition_penalty=1.0, num_beams=1, fsdp='', fsdp_config=None, sequence_parallel_size=1, model_layer_cls_name=None, metric_warmup_step=0, fsdp_num=1, per_device_train_batch_size=None, per_device_eval_batch_size=None, eval_strategy=None, self_cognition_sample=0, train_dataset_mix_ratio=0.0, train_dataset_mix_ds=['ms-bench'], train_dataset_sample=-1, val_dataset_sample=None, safe_serialization=None, only_save_model=None, neftune_alpha=None, deepspeed_config_path=None, model_cache_dir=None, custom_train_dataset_path=[], custom_val_dataset_path=[])\n",
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: qwen/Qwen-1_8B-Chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_count: 1\n",
      "rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING:modelscope] Using the master branch is fragile, please use it with caution!\n",
      "[INFO:modelscope] Use user-specified model revision: master\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat\n",
      "[INFO:swift] model_kwargs: {'low_cpu_mem_usage': True, 'device_map': 'cuda:0'}\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0254fac2664b4ee5b2bda216c390d7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] model.max_model_len: 8192\n",
      "[INFO:swift] model_config: QWenConfig {\n",
      "  \"_name_or_path\": \"/mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat\",\n",
      "  \"architectures\": [\n",
      "    \"QWenLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_dropout_prob\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_qwen.QWenConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_qwen.QWenLMHeadModel\"\n",
      "  },\n",
      "  \"bf16\": true,\n",
      "  \"emb_dropout_prob\": 0.0,\n",
      "  \"fp16\": false,\n",
      "  \"fp32\": false,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"kv_channels\": 128,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"qwen\",\n",
      "  \"no_bias\": true,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"onnx_safe\": null,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 1.0,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"seq_length\": 8192,\n",
      "  \"softmax_in_fp32\": false,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"QWenTokenizer\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_cache_kernel\": false,\n",
      "  \"use_cache_quantization\": false,\n",
      "  \"use_dynamic_ntk\": true,\n",
      "  \"use_flash_attn\": true,\n",
      "  \"use_logn_attn\": true,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO:swift] generation_config: GenerationConfig {\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"pad_token_id\": 151645,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.7\n",
      "}\n",
      "\n",
      "[INFO:swift] lora_target_modules: ['w1', 'w2', 'c_attn', 'c_proj']\n",
      "[INFO:swift] lora_modules_to_save: []\n",
      "[INFO:swift] lora_config: get_wrapped_class.<locals>.PeftWrapper(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=8, target_modules={'w1', 'w2', 'c_attn', 'c_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] [base_model.model.transformer.wte.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.ln_1.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.attn.c_attn.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.attn.c_attn.base_layer.bias]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.attn.c_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.attn.c_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.attn.c_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.ln_2.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.w1.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.w1.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.w1.lora_B.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.w2.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.w2.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.w2.lora_B.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.c_proj.base_layer.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.c_proj.lora_A.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.0.mlp.c_proj.lora_B.default.weight]: requires_grad=True, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] [base_model.model.transformer.h.1.ln_1.weight]: requires_grad=False, dtype=torch.bfloat16, device=cuda:0\n",
      "[INFO:swift] ...\n",
      "[INFO:swift] PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): QWenLMHeadModel(\n",
      "      (transformer): QWenModel(\n",
      "        (wte): Embedding(151936, 2048)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x QWenBlock(\n",
      "            (ln_1): RMSNorm()\n",
      "            (attn): QWenAttention(\n",
      "              (c_attn): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=6144, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (core_attention_flash): FlashSelfAttention()\n",
      "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ln_2): RMSNorm()\n",
      "            (mlp): QWenMLP(\n",
      "              (w1): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5504, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (w2): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5504, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (c_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5504, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): RMSNorm()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] PeftModelForCausalLM: 1843.5379M Params (6.7092M Trainable [0.3639%]), 0.7865M Buffers.\n",
      "[INFO:swift] Setting model.config.use_cache: False\n",
      "[INFO:swift] Downloading the dataset from ModelScope, dataset_id: AI-ModelScope/alpaca-gpt4-data-zh\n",
      "[INFO:modelscope] Downloading to /mnt/workspace/.cache/modelscope/hub/datasets/0e5068922103f4f9417b7739909a0715b01750431f2038e5f4e9d7dcdd6cdcaa.incomplete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f14be71cb345f495115910de82ba1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] storing https://www.modelscope.cn/api/v1/datasets/AI-ModelScope/alpaca-gpt4-data-zh/repo?Source=SDK&Revision=master&FilePath=README.md&View=False in cache at /mnt/workspace/.cache/modelscope/hub/datasets/0e5068922103f4f9417b7739909a0715b01750431f2038e5f4e9d7dcdd6cdcaa\n",
      "[INFO:modelscope] creating metadata file for /mnt/workspace/.cache/modelscope/hub/datasets/0e5068922103f4f9417b7739909a0715b01750431f2038e5f4e9d7dcdd6cdcaa\n",
      "[INFO:modelscope] Downloading to /mnt/workspace/.cache/modelscope/hub/datasets/downloads/ee3959cc16ee530c43270b123e2d8694a153a70d1b9a10d1e697df701b3fd791.incomplete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec85444af0c4a7ab96d6f753a2115c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] storing https://www.modelscope.cn/api/v1/datasets/AI-ModelScope/alpaca-gpt4-data-zh/repo?Source=SDK&Revision=master&FilePath=train.csv in cache at /mnt/workspace/.cache/modelscope/hub/datasets/downloads/ee3959cc16ee530c43270b123e2d8694a153a70d1b9a10d1e697df701b3fd791\n",
      "[INFO:modelscope] creating metadata file for /mnt/workspace/.cache/modelscope/hub/datasets/downloads/ee3959cc16ee530c43270b123e2d8694a153a70d1b9a10d1e697df701b3fd791\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5632bb09aec42dc8966693d075c8d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Context manager of ms-dataset exited.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ed99524423481fbb0e6ebb58e4de6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306a37855fea42beaf2c94ed1008159d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/495 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2a2467843d451da9f8f77d85cbc810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bd4f65dbfc4080b476e2c00f836ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed559d9b33f54cbc83df409aede34f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c354656c7448538b6d419b388f5362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 994\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 6\n",
      "})\n",
      "[INFO:swift] system: You are a helpful assistant.\n",
      "[INFO:swift] args.lazy_tokenize: False\n",
      "[INFO:swift] Using num_proc: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e883ec476546dab4fc4d5d212f7f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb234baa85b420dbcfeae744042242f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] [INPUT_IDS] [151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 101042, 87752, 20074, 62926, 101987, 103929, 106073, 8997, 32664, 16, 15, 99605, 103993, 101923, 3837, 56007, 99650, 100399, 104602, 99998, 100019, 1773, 22, 99605, 36587, 100399, 104602, 3837, 18, 99605, 36587, 116889, 1773, 151645, 198, 151644, 77091, 198, 106073, 5122, 100345, 101923, 59151, 3837, 16, 15, 99605, 105656, 22, 15, 4, 100623, 99729, 100399, 104602, 3837, 118233, 102250, 101043, 18, 15, 4, 100623, 99729, 116889, 1773, 43288, 102406, 104596, 101923, 110241, 15946, 3837, 100399, 104602, 100623, 56006, 116889, 100623, 42140, 1773, 100169, 17447, 3837, 104602, 33108, 100019, 99250, 102212, 100140, 20412, 101441, 100218, 106099, 3837, 102527, 73670, 83751, 63789, 96050, 99487, 101923, 110241, 15946, 3837, 104602, 9370, 103215, 100069, 105688, 100019, 1773, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "分析以下数据并展示你的结论。\n",
      "对10个人进行了调查，问他们喝咖啡还是茶。7个人说喝咖啡，3个人说喝茶。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "结论：根据调查结果，10个人中有70%的人喜欢喝咖啡，相较之下只有30%的人喜欢喝茶。这表明在这个调查样本中，喝咖啡的人比喝茶的人多。传统上，咖啡和茶被普遍认为是两种竞争饮料，由此可以推断，在这个调查样本中，咖啡的受欢迎程度高于茶。<|im_end|>\n",
      "[INFO:swift] [LABLES_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 106073, 5122, 100345, 101923, 59151, 3837, 16, 15, 99605, 105656, 22, 15, 4, 100623, 99729, 100399, 104602, 3837, 118233, 102250, 101043, 18, 15, 4, 100623, 99729, 116889, 1773, 43288, 102406, 104596, 101923, 110241, 15946, 3837, 100399, 104602, 100623, 56006, 116889, 100623, 42140, 1773, 100169, 17447, 3837, 104602, 33108, 100019, 99250, 102212, 100140, 20412, 101441, 100218, 106099, 3837, 102527, 73670, 83751, 63789, 96050, 99487, 101923, 110241, 15946, 3837, 104602, 9370, 103215, 100069, 105688, 100019, 1773, 151645]\n",
      "[INFO:swift] [LABLES] [-100 * 52]结论：根据调查结果，10个人中有70%的人喜欢喝咖啡，相较之下只有30%的人喜欢喝茶。这表明在这个调查样本中，喝咖啡的人比喝茶的人多。传统上，咖啡和茶被普遍认为是两种竞争饮料，由此可以推断，在这个调查样本中，咖啡的受欢迎程度高于茶。<|im_end|>\n",
      "[INFO:swift] Dataset Token Length: 108.544266±85.340722, min=33.000000, max=525.000000, size=994\n",
      "[INFO:swift] Dataset Token Length: 179.666667±89.917864, min=57.000000, max=300.000000, size=6\n",
      "[INFO:swift] training_args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "additional_saved_files=[],\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=1,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=GenerationConfig {\n",
      "  \"chat_format\": \"chatml\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"max_new_tokens\": 2048,\n",
      "  \"max_window_size\": 6144,\n",
      "  \"pad_token_id\": 151645,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.7\n",
      "}\n",
      ",\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=16,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=0.5,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_hub_strategy=push_best,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=False,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataset_sample=994,\n",
      "train_sampler_random=True,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      ")\n",
      "[INFO:swift] Model file cli_demo.gif is different from the latest version `v1.0.0`,This is because you are using an older version or the file is updated manually.\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO:swift] The SftArguments will be saved in: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/sft_args.json\n",
      "[INFO:swift] The Seq2SeqTrainingArguments will be saved in: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/training_args.json\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/logging.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-23 15:57:54,451] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: 没有那个文件或目录\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a92adeb194f4f0ebe99ed35c529d756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.36874175, 'acc': 0.47317716, 'grad_norm': 3.203125, 'learning_rate': 2.5e-05, 'memory(GiB)': 11.46, 'train_speed(iter/s)': 0.252451, 'epoch': 0.02, 'global_step/max_steps': '1/62', 'percentage': '1.61%', 'elapsed_time': '3s', 'remaining_time': '3m 10s'}\n",
      "{'loss': 2.1540308, 'acc': 0.52516752, 'grad_norm': 2.03125, 'learning_rate': 9.993e-05, 'memory(GiB)': 11.86, 'train_speed(iter/s)': 0.313256, 'epoch': 0.08, 'global_step/max_steps': '5/62', 'percentage': '8.06%', 'elapsed_time': '15s', 'remaining_time': '2m 52s'}\n",
      "{'loss': 1.86708889, 'acc': 0.58053837, 'grad_norm': 1.5390625, 'learning_rate': 9.738e-05, 'memory(GiB)': 12.72, 'train_speed(iter/s)': 0.328026, 'epoch': 0.16, 'global_step/max_steps': '10/62', 'percentage': '16.13%', 'elapsed_time': '29s', 'remaining_time': '2m 34s'}\n",
      "{'loss': 1.61841278, 'acc': 0.59912338, 'grad_norm': 1.6328125, 'learning_rate': 9.138e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.333229, 'epoch': 0.24, 'global_step/max_steps': '15/62', 'percentage': '24.19%', 'elapsed_time': '44s', 'remaining_time': '2m 18s'}\n",
      "{'loss': 1.47101507, 'acc': 0.63088241, 'grad_norm': 1.25, 'learning_rate': 8.237e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.333929, 'epoch': 0.32, 'global_step/max_steps': '20/62', 'percentage': '32.26%', 'elapsed_time': '59s', 'remaining_time': '2m 4s'}\n",
      "{'loss': 1.38644953, 'acc': 0.64821048, 'grad_norm': 1.3828125, 'learning_rate': 7.099e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.335751, 'epoch': 0.4, 'global_step/max_steps': '25/62', 'percentage': '40.32%', 'elapsed_time': '1m 13s', 'remaining_time': '1m 48s'}\n",
      "{'loss': 1.50699949, 'acc': 0.62897868, 'grad_norm': 1.4765625, 'learning_rate': 5.809e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.336923, 'epoch': 0.48, 'global_step/max_steps': '30/62', 'percentage': '48.39%', 'elapsed_time': '1m 28s', 'remaining_time': '1m 34s'}\n",
      "{'loss': 1.19713335, 'acc': 0.69465842, 'grad_norm': 1.5625, 'learning_rate': 4.459e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.338135, 'epoch': 0.56, 'global_step/max_steps': '35/62', 'percentage': '56.45%', 'elapsed_time': '1m 42s', 'remaining_time': '1m 19s'}\n",
      "{'loss': 1.22061281, 'acc': 0.67961745, 'grad_norm': 1.5, 'learning_rate': 3.149e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.338637, 'epoch': 0.64, 'global_step/max_steps': '40/62', 'percentage': '64.52%', 'elapsed_time': '1m 57s', 'remaining_time': '1m 4s'}\n",
      "{'loss': 1.31485653, 'acc': 0.67527261, 'grad_norm': 1.125, 'learning_rate': 1.974e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.338046, 'epoch': 0.72, 'global_step/max_steps': '45/62', 'percentage': '72.58%', 'elapsed_time': '2m 12s', 'remaining_time': '49s'}\n",
      "{'loss': 1.22076635, 'acc': 0.69620867, 'grad_norm': 1.4453125, 'learning_rate': 1.02e-05, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.338368, 'epoch': 0.8, 'global_step/max_steps': '50/62', 'percentage': '80.65%', 'elapsed_time': '2m 26s', 'remaining_time': '35s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655fff917f9e4bbcaaef8834ae1e1266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/checkpoint-50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.40224171, 'eval_acc': 0.62159215, 'eval_runtime': 0.3738, 'eval_samples_per_second': 16.051, 'eval_steps_per_second': 16.051, 'epoch': 0.8, 'global_step/max_steps': '50/62', 'percentage': '80.65%', 'elapsed_time': '2m 27s', 'remaining_time': '35s'}\n",
      "{'loss': 1.20385685, 'acc': 0.70072217, 'grad_norm': 1.359375, 'learning_rate': 3.55e-06, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.337627, 'epoch': 0.89, 'global_step/max_steps': '55/62', 'percentage': '88.71%', 'elapsed_time': '2m 42s', 'remaining_time': '20s'}\n",
      "{'loss': 1.30000114, 'acc': 0.67061954, 'grad_norm': 1.4765625, 'learning_rate': 2.9e-07, 'memory(GiB)': 13.32, 'train_speed(iter/s)': 0.337736, 'epoch': 0.97, 'global_step/max_steps': '60/62', 'percentage': '96.77%', 'elapsed_time': '2m 56s', 'remaining_time': '5s'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc44678a58941e89d88be70f436cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/checkpoint-62\n",
      "[INFO:swift] last_model_checkpoint: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/checkpoint-62\n",
      "[INFO:swift] best_model_checkpoint: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/checkpoint-50\n",
      "[INFO:swift] images_dir: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/images\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.40483391, 'eval_acc': 0.62377317, 'eval_runtime': 0.3784, 'eval_samples_per_second': 15.855, 'eval_steps_per_second': 15.855, 'epoch': 1.0, 'global_step/max_steps': '62/62', 'percentage': '100.00%', 'elapsed_time': '3m 3s', 'remaining_time': '0s'}\n",
      "{'train_runtime': 183.4146, 'train_samples_per_second': 5.419, 'train_steps_per_second': 0.338, 'train_loss': 1.44525413, 'epoch': 1.0, 'global_step/max_steps': '62/62', 'percentage': '100.00%', 'elapsed_time': '3m 3s', 'remaining_time': '0s'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] End time of running main: 2024-07-23 16:01:00.260616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_checkpoint: /mnt/workspace/swift/output/qwen-1_8b-chat/v2-20240723-155740/checkpoint-50\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from swift.llm import DatasetName, ModelType, SftArguments, sft_main\n",
    "\n",
    "sft_args = SftArguments(\n",
    "    model_type=ModelType.qwen_1_8b_chat,\n",
    "    dataset=[f'{DatasetName.alpaca_zh}#500',\n",
    "             f'kenan.json#500'],\n",
    "    logging_steps=5,\n",
    "    max_length=2048,\n",
    "    learning_rate=1e-4,\n",
    "    output_dir='output',\n",
    "    lora_target_modules=['ALL']\n",
    "    )\n",
    "output = sft_main(sft_args)\n",
    "best_model_checkpoint = output['best_model_checkpoint']\n",
    "print(f'best_model_checkpoint: {best_model_checkpoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617a7e06-4cda-40cc-afa8-61f760c4fe43",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-23T08:02:15.784096Z",
     "iopub.status.busy": "2024-07-23T08:02:15.783480Z",
     "iopub.status.idle": "2024-07-23T08:02:20.951517Z",
     "shell.execute_reply": "2024-07-23T08:02:20.951035Z",
     "shell.execute_reply.started": "2024-07-23T08:02:15.784072Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: qwen/Qwen-1_8B-Chat\n",
      "[WARNING:modelscope] Using the master branch is fragile, please use it with caution!\n",
      "[INFO:modelscope] Use user-specified model revision: master\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat\n",
      "[INFO:swift] Setting torch_dtype: torch.float16\n",
      "[INFO:swift] model_kwargs: {'device_map': 'auto'}\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1281f28e79644630be595212d7f961ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] model.max_model_len: 8192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: 您好，我是 柯南，一个由 中科大 开发的人工智能助手。我可以回答各种问题、提供信息和执行任务，以帮助用户解决问题并满足他们的需求。\n",
      "history: [['介绍你自己', '您好，我是 柯南，一个由 中科大 开发的人工智能助手。我可以回答各种问题、提供信息和执行任务，以帮助用户解决问题并满足他们的需求。']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "from swift.llm import (\n",
    "    get_model_tokenizer, get_template, inference, ModelType, get_default_template_type,\n",
    ")\n",
    "from swift.utils import seed_everything\n",
    "from swift.tuners import Swift\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "ckpt_dir = 'output/qwen-1_8b-chat/v2-20240723-155740/checkpoint-62'\n",
    "model_type=ModelType.qwen_1_8b_chat\n",
    "\n",
    "template_type = get_default_template_type(model_type)\n",
    "\n",
    "model, tokenizer = get_model_tokenizer(model_type, model_kwargs={'device_map': 'auto'})\n",
    "model.generation_config.max_new_tokens = 128\n",
    "\n",
    "model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)\n",
    "template = get_template(template_type, tokenizer)\n",
    "\n",
    "query = '介绍你自己'\n",
    "response, history = inference(model, template, query)\n",
    "print(f'response: {response}')\n",
    "print(f'history: {history}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
