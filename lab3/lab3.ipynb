{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备\n",
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric import transforms\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 功能函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_edge(edge_index, p=0.5, training=True):\n",
    "    if not training:\n",
    "        return edge_index\n",
    "    mask = torch.rand(edge_index.size(1), device=edge_index.device) > p\n",
    "    return edge_index[:, mask]\n",
    "\n",
    "def eval_acc(model, data, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data).argmax(dim=1)\n",
    "        correct = (out[mask] == data.y[mask])\n",
    "    return correct.sum().item() / mask.sum().item()\n",
    "\n",
    "def node_classifier(model, optimizer, criterion, data, num_epochs = 5, \n",
    "                    scheduler=None, comment=None):\n",
    "    def accuracy(pred, mask):\n",
    "        correct = (pred[mask] == data.y[mask])\n",
    "        acc = correct.sum().item() / mask.sum().item()\n",
    "        return acc\n",
    "    def train_step():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = accuracy(out.argmax(dim=1), data.train_mask)\n",
    "        return loss, acc\n",
    "    def val_step():\n",
    "        model.eval()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "        pred = out.argmax(dim=1)\n",
    "        acc = accuracy(pred, data.val_mask)\n",
    "        return loss, acc\n",
    "    \n",
    "    writer = SummaryWriter(comment=comment)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss, train_acc = train_step()\n",
    "        val_loss, val_acc = val_step()\n",
    "        scheduler.step(train_loss)\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        writer.add_scalars(main_tag='Loss',\n",
    "                          tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                           \"validation_loss\": val_loss}, \n",
    "                          global_step=epoch)\n",
    "        writer.add_scalars(main_tag='Accuracy',\n",
    "                          tag_scalar_dict={\"validation_acc\": val_acc,\n",
    "                                           \"train_acc\": train_acc}, \n",
    "                          global_step=epoch)\n",
    "        writer.add_scalars(main_tag=\"lr\", tag_scalar_dict={'lr':lr}, global_step=epoch)\n",
    "    writer.close()\n",
    "\n",
    "def eval_auc(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "        out = model.decode(z, data.edge_label_index).view(-1).sigmoid()\n",
    "        model.train()\n",
    "    return roc_auc_score(data.edge_label.cpu().numpy(), out.cpu().numpy())\n",
    " \n",
    "def link_predictor(model, optimizer, criterion, train_data, val_data, num_epochs=5, \n",
    "                   scheduler=None, comment=None):\n",
    "\n",
    "    def train_step():\n",
    "        model.train()\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=train_data.edge_index, \n",
    "            num_nodes=train_data.num_nodes,\n",
    "            num_neg_samples=train_data.edge_label_index.size(1), \n",
    "            method='sparse'\n",
    "        ).to(device)\n",
    "        edge_label_index = torch.cat(\n",
    "            [train_data.edge_label_index, neg_edge_index],\n",
    "            dim=-1,\n",
    "        )\n",
    "        edge_label = torch.cat([\n",
    "            train_data.edge_label,\n",
    "            train_data.edge_label.new_zeros(neg_edge_index.size(1))\n",
    "        ], dim=0)\n",
    "        optimizer.zero_grad()\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "        out = model.decode(z, edge_label_index).view(-1)\n",
    "        loss = criterion(out, edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        auc_score = roc_auc_score(edge_label.cpu().numpy(), out.sigmoid().detach().cpu().numpy())\n",
    "        return loss, auc_score\n",
    "    \n",
    "    def val_step():\n",
    "        model.eval()\n",
    "        z = model.encode(val_data.x, val_data.edge_index)\n",
    "        out = model.decode(z, val_data.edge_label_index).view(-1)\n",
    "        loss = criterion(out, val_data.edge_label)\n",
    "        auc_score = eval_auc(model, val_data)\n",
    "        return loss, auc_score\n",
    "    \n",
    "    \n",
    "    writer = SummaryWriter(comment=comment)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss, train_auc = train_step()\n",
    "        val_loss, val_auc = val_step()\n",
    "        scheduler.step(train_loss)\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        writer.add_scalars(main_tag='Loss',\n",
    "                          tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                           \"validation_loss\": val_loss}, \n",
    "                          global_step=epoch)\n",
    "        writer.add_scalars(main_tag='AUC',\n",
    "                          tag_scalar_dict={\"validation_auc\": val_auc,\n",
    "                                           \"train_auc\": train_auc},\n",
    "                          global_step=epoch)\n",
    "        writer.add_scalars(main_tag=\"lr\", tag_scalar_dict={'lr':lr}, global_step=epoch)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myPairNorm(nn.Module):\n",
    "    def __init__(self, scale=1., epsilon=1e-5):\n",
    "        super(myPairNorm, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.eps = epsilon\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x - x.mean(dim=0, keepdim=True)\n",
    "        x = self.scale * x / (x.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myGCNConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, self_loop=True):\n",
    "        super(myGCNConv, self).__init__()\n",
    "        self.loop = self_loop\n",
    "        self.A_hat = None\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "    \n",
    "    def get_A_hat(self, edge_index, num_nodes):\n",
    "        A = torch.zeros((num_nodes, num_nodes), dtype=torch.float)\n",
    "        for i in range(edge_index.size(1)):\n",
    "            src, dst = edge_index[:, i]\n",
    "            A[src, dst] = 1\n",
    "            A[dst, src] = 1\n",
    "        if self.loop:\n",
    "            I = torch.diag(torch.ones(num_nodes))\n",
    "            A = A + I\n",
    "        D = torch.diag(torch.sum(A, dim=1))\n",
    "        A_hat = torch.inverse(torch.sqrt(D)) @ A @ torch.inverse(torch.sqrt(D))\n",
    "        return A_hat.to(device)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if self.A_hat == None:\n",
    "            self.A_hat = self.get_A_hat(edge_index, x.size(0))\n",
    "        x = self.lin(x)\n",
    "        x = torch.matmul(self.A_hat, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, features, hidden_dimension, hiddel_layers, classes, self_loop=True, drop_edge=False, pair_norm=False, activation=nn.ReLU()):\n",
    "        super(GCN, self).__init__()\n",
    "        # 输入的节点特征维度 * 中间隐藏层的维度\n",
    "        self.net = nn.ModuleList()\n",
    "        for i in range(hiddel_layers):\n",
    "            if i == 0:\n",
    "                self.net.append(myGCNConv(features, hidden_dimension, self_loop=self_loop))\n",
    "            else:\n",
    "                self.net.append(myGCNConv(hidden_dimension, hidden_dimension, self_loop=self_loop))\n",
    "        # 中间隐藏层的维度 * 节点类别\n",
    "        self.last_conv = myGCNConv(hidden_dimension, classes, self_loop=self_loop)\n",
    "        self.norm_layer = myPairNorm()\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.drop_edge = drop_edge\n",
    "        self.pair_norm = pair_norm\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        if self.drop_edge and self.training:\n",
    "            edge_index = dropout_edge(edge_index, p=0.5)\n",
    "        for conv in self.net:\n",
    "            x = self.activation(conv(x, edge_index))\n",
    "            if self.pair_norm:\n",
    "                x = self.norm_layer(x)\n",
    "        x = self.last_conv(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, z, edge_label_index):\n",
    "        # z所有节点的表示向量\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        # print(dst.size())   # (7284, 64)\n",
    "        r = (src * dst).sum(dim=-1)\n",
    "        # print(r.size())   (7284)\n",
    "        return r\n",
    "\n",
    "    def forward(self, data):\n",
    "        # 节点特征 和 邻接关系\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # 编码\n",
    "        x = self.encode(x, edge_index)\n",
    "        # 使用 softmax 得到概率分布\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节点分类\n",
    "### 数据加载\n",
    "split = 'full' 可以让所有不在val和test的结点加入train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Number of training nodes: 1208\n",
      "Number of validation nodes: 500\n",
      "Number of test nodes: 1000\n",
      "Training node label rate: 0.45\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n",
      "e:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'contains_self_loops' is deprecated, use 'has_self_loops' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/Cora', name='Cora', transform=transforms.NormalizeFeatures(), split='full')\n",
    "data = dataset[0].to(device)\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}\\n')\n",
    "\n",
    "print(data)\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
    "print(f'Number of test nodes: {data.test_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fcaf776a30445cf845f0cd815edcff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GCN(dataset.num_features, 64, 1, dataset.num_classes, self_loop=True, drop_edge=False, pair_norm=False).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "node_classifier(model, optimizer, criterion, data, num_epochs=300, scheduler=scheduler, comment='_Cora+loop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.877"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_acc(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链路预测\n",
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.NormalizeFeatures(),\n",
    "    transforms.ToDevice(device),\n",
    "    transforms.RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "dataset = Planetoid(root='data/Cora', name='Cora', transform=transform, split='full')\n",
    "\n",
    "train_data, val_data, test_data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2913c6d57efd4225a9d5651cbffa70dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GCN(dataset.num_features, 128, 1, 64, self_loop=True, drop_edge=True, pair_norm=True, activation=nn.Sigmoid()).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "link_predictor(model, optimizer, criterion, train_data, val_data, num_epochs=300, scheduler=scheduler, comment='_Cora+de+pn+sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9430560006337114"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_auc(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citeseer\n",
    "## 节点分类\n",
    "### 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: CiteSeer():\n",
      "======================\n",
      "Number of graphs: 1\n",
      "Number of features: 3703\n",
      "Number of classes: 6\n",
      "\n",
      "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n",
      "Number of nodes: 3327\n",
      "Number of edges: 9104\n",
      "Average node degree: 2.74\n",
      "Number of training nodes: 1827\n",
      "Number of validation nodes: 500\n",
      "Number of test nodes: 1000\n",
      "Training node label rate: 0.55\n",
      "Contains isolated nodes: True\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data', name=\"CiteSeer\", split='full')\n",
    "data = dataset[0].to(device)\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}\\n')\n",
    "\n",
    "print(data)\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Number of validation nodes: {data.val_mask.sum()}')\n",
    "print(f'Number of test nodes: {data.test_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bac8cd24bc4eb5b1fb175b2bc76ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GCN(dataset.num_features, 64, 1, dataset.num_classes, self_loop=True, drop_edge=True, pair_norm=False).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "node_classifier(model, optimizer, criterion, data, num_epochs=400, scheduler=scheduler, comment='_CiteSeer+de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.729"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_acc(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 链路预测\n",
    "### 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.NormalizeFeatures(),\n",
    "    transforms.ToDevice(device),\n",
    "    transforms.RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=True,\n",
    "                      add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "dataset = Planetoid(root='data', name='Citeseer', transform=transform, split='full')\n",
    "\n",
    "train_data, val_data, test_data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\envs\\d2l\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d0f689c89b40aba7f48b17d2b31f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GCN(dataset.num_features, 64, 1, 64, drop_edge=True, pair_norm=True).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "link_predictor(model, optimizer, criterion, train_data, val_data, num_epochs=200, scheduler=scheduler, comment='_Citeseer+de+pn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922043231493781"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_auc(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.NormalizeFeatures(),\n",
    "    # transforms.ToDevice(device),\n",
    "    # transforms.RandomLinkSplit(num_val=0.1, num_test=0.1, is_undirected=True,\n",
    "                      # add_negative_train_samples=False),\n",
    "])\n",
    "\n",
    "dataset = Planetoid(root='data', name='Citeseer', transform=transform, split='full')\n",
    "\n",
    "train_data = dataset[0]\n",
    "data = train_data\n",
    "\n",
    "A = torch.zeros((data.num_nodes, data.num_nodes), dtype=torch.float)\n",
    "for i in range(data.edge_index.size(1)):\n",
    "    src, dst = data.edge_index[:, i]\n",
    "    A[src, dst] = 1\n",
    "    A[dst, src] = 1\n",
    "D = torch.diag(torch.sum(A, dim=1))\n",
    "for i in range(D.shape[0]):\n",
    "    if D[i, i] == 0:\n",
    "        print(i)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[192][192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/Cora', name='Cora', transform=transforms.NormalizeFeatures(), split='full')\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 2, 2, 3],\n",
       "        [1, 0, 2, 1, 3, 2]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor([[0, 1, 1, 2, 2, 3],\n",
    "                           [1, 0, 2, 1, 3, 2]])\n",
    "edge_index = dropout_edge(edge_index, p=0.25, training=True)\n",
    "edge_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
